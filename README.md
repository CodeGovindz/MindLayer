<div align="center">

# ğŸ§  Universal Memory Layer

**Seamless conversation continuity across multiple AI models**

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)
[![Tests](https://img.shields.io/badge/Tests-Passing-brightgreen?style=for-the-badge&logo=pytest)](tests/)
[![Documentation](https://img.shields.io/badge/Docs-Complete-blue?style=for-the-badge&logo=gitbook)](docs/)

[![OpenAI](https://img.shields.io/badge/OpenAI-GPT--4%20%7C%20GPT--3.5-412991?style=flat-square&logo=openai)](https://openai.com)
[![Google](https://img.shields.io/badge/Google-Gemini-4285F4?style=flat-square&logo=google)](https://ai.google.dev)
[![Anthropic](https://img.shields.io/badge/Anthropic-Claude-D4A574?style=flat-square)](https://anthropic.com)
[![Groq](https://img.shields.io/badge/Groq-Llama%20%7C%20Mixtral-FF6B35?style=flat-square)](https://groq.com)

*Switch between ChatGPT, Claude, Gemini, and Groq models while maintaining perfect conversation context*

</div>

---

## âœ¨ Features

ğŸ”„ **Multi-LLM Support** - Seamlessly switch between OpenAI, Google, Anthropic, and Groq models  
ğŸ’¾ **Persistent Memory** - Store conversation history with metadata across different providers  
ğŸ” **Semantic Search** - Find relevant messages using vector embeddings for intelligent context retrieval  
ğŸ¯ **Unified Interface** - Consistent API for interacting with different LLM providers  
ğŸ’¬ **Interactive CLI** - Easy-to-use command-line interface for conversations  
âš™ï¸ **Configurable Embeddings** - Support for OpenAI and Hugging Face embedding providers  
ğŸ§© **Context Strategies** - Multiple strategies for retrieving conversation context (recent, relevant, hybrid)  
ğŸ›¡ï¸ **Error Handling** - Graceful handling of API failures and rate limits

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/CodeGovindz/MindLayer.git
cd MindLayer

# Install dependencies
pip install -r requirements.txt

# Install the package
pip install -e .
```

### API Keys Setup

Set up your API keys (at least one required):

```bash
# OpenAI (for ChatGPT, GPT-4)
export OPENAI_API_KEY="sk-your-openai-key"

# Google (for Gemini)
export GOOGLE_API_KEY="AIza-your-google-key"

# Anthropic (for Claude)
export ANTHROPIC_API_KEY="sk-ant-your-anthropic-key"

# Groq (for Llama, Mixtral)
export GROQ_API_KEY="gsk-your-groq-key"
```

### Launch

```bash
# Interactive CLI
python -m universal_memory_layer.cli.main

# Or use the console script
uml
```

## ğŸ¯ Supported Models

| Provider | Models | Status |
|----------|--------|--------|
| **OpenAI** | GPT-4, GPT-3.5-turbo, GPT-5-nano | âœ… Supported |
| **Google** | Gemini Pro, Gemini-2.0-flash | âœ… Supported |
| **Anthropic** | Claude-3 Sonnet | âœ… Supported |
| **Groq** | Llama-3.1-8B, Mixtral-8x7B | âœ… Supported |

## ğŸ’¡ Usage Examples

### Interactive CLI

```bash
# Start the CLI
python -m universal_memory_layer.cli.main

# Available commands:
/help              # Show all commands
/models            # List available models  
/switch            # Switch between models
/recent 5          # Show last 5 messages
/search "topic"    # Search conversation history
/stats             # Show memory statistics
/quit              # Exit
```

### Programmatic Usage

```python
from universal_memory_layer.conversation_manager import ConversationManager

# Initialize
manager = ConversationManager()
manager.initialize()

# Chat with GPT-4
manager.switch_model("gpt-4")
response = manager.chat("Hello! How are you?")

# Switch to Gemini while maintaining context
manager.switch_model("gemini-2.0-flash")
response = manager.chat("What did we just talk about?")

# Use different context strategies
response = manager.chat(
    "Tell me more about that topic",
    context_strategy="relevant",  # or "recent", "hybrid"
    context_count=5
)
```

### Model Switching Demo

```python
# Available models
models = manager.get_available_models()
# ['chatgpt', 'gpt-4', 'gemini', 'gemini-2.0-flash', 'claude', 'llama-3.1-8b', 'mixtral-8x7b']

# Seamless switching with memory retention
manager.switch_model("llama-3.1-8b")  # Groq
response1 = manager.chat("I love machine learning")

manager.switch_model("gemini-2.0-flash")  # Google  
response2 = manager.chat("What do I love?")  # Remembers context!
```

## âš™ï¸ Configuration

### Environment Variables

| Variable | Description | Default | Example |
|----------|-------------|---------|---------|
| `OPENAI_API_KEY` | OpenAI API key | - | `sk-proj-...` |
| `GOOGLE_API_KEY` | Google AI API key | - | `AIza...` |
| `ANTHROPIC_API_KEY` | Anthropic API key | - | `sk-ant-...` |
| `GROQ_API_KEY` | Groq API key | - | `gsk_...` |
| `UML_EMBEDDING_PROVIDER` | Embedding provider | `openai` | `huggingface` |
| `UML_DATABASE_PATH` | Database file path | `memory.db` | `./data/memory.db` |
| `UML_LOG_LEVEL` | Logging level | `WARNING` | `INFO` |

### Configuration File

Create a `.env` file for easy setup:

```bash
# API Keys
OPENAI_API_KEY=sk-your-openai-key
GOOGLE_API_KEY=AIza-your-google-key
GROQ_API_KEY=gsk-your-groq-key

# Settings
UML_EMBEDDING_PROVIDER=huggingface
UML_LOG_LEVEL=INFO
```

## ğŸ—ï¸ Architecture

```mermaid
graph TB
    CLI[ğŸ–¥ï¸ CLI Interface] --> CM[ğŸ§  Conversation Manager]
    API[ğŸ”Œ Python API] --> CM
    
    CM --> LLM1[ğŸ¤– OpenAI Client]
    CM --> LLM2[ğŸ¤– Google Client] 
    CM --> LLM3[ğŸ¤– Anthropic Client]
    CM --> LLM4[ğŸ¤– Groq Client]
    
    CM --> MS[ğŸ’¾ Memory Store]
    MS --> DB[(ğŸ—„ï¸ SQLite Database)]
    MS --> VS[ğŸ” Vector Store]
    MS --> EMB[ğŸ“Š Embeddings]
    
    EMB --> OPENAI[OpenAI Embeddings]
    EMB --> HF[ğŸ¤— Hugging Face]
```

## ğŸ“ Project Structure

```
universal_memory_layer/
â”œâ”€â”€ ğŸ§  models/              # Data models (Message, Config)
â”œâ”€â”€ ğŸ’¾ storage/             # Database & vector storage
â”œâ”€â”€ ğŸ¤– clients/             # LLM client implementations  
â”œâ”€â”€ ğŸ“Š embeddings/          # Embedding providers
â”œâ”€â”€ ğŸ–¥ï¸ cli/                # Command-line interface
â”œâ”€â”€ ğŸ”§ config_loader.py    # Configuration management
â”œâ”€â”€ ğŸ“ logging_config.py   # Logging setup
â””â”€â”€ âš¡ conversation_manager.py  # Main orchestrator
```

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=universal_memory_layer

# Run integration tests
python tests/run_integration_tests.py

# Test specific components
pytest tests/test_conversation_manager.py
pytest tests/storage/
pytest tests/clients/
```

## ğŸ“Š Performance

| Operation | Time | Memory |
|-----------|------|--------|
| Model switching | <100ms | ~50MB |
| Message storage | <50ms | ~10MB |
| Context retrieval | <200ms | ~20MB |
| Semantic search | <500ms | ~100MB |

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“‹ Requirements

- **Python**: 3.8+
- **Dependencies**: See [requirements.txt](requirements.txt)
- **API Keys**: At least one LLM provider
- **Storage**: 100MB+ free space

## ğŸ”’ Security

- API keys are never logged or stored
- Local database encryption available
- Secure memory handling
- Rate limiting and error handling

## ğŸ› Troubleshooting

<details>
<summary><b>ğŸš« No Models Available</b></summary>

**Problem**: CLI shows "No models available"

**Solutions**:
- âœ… Set at least one API key in environment variables
- âœ… Verify API key format and validity  
- âœ… Check network connectivity

```bash
# Check if keys are set
echo $OPENAI_API_KEY
echo $GOOGLE_API_KEY
```
</details>

<details>
<summary><b>ğŸ’¾ Database Errors</b></summary>

**Problem**: SQLite database errors

**Solutions**:
- âœ… Check file permissions for database directory
- âœ… Ensure sufficient disk space
- âœ… Delete database file to recreate: `rm memory.db`
</details>

<details>
<summary><b>ğŸ” Embedding Issues</b></summary>

**Problem**: Embedding generation fails

**Solutions**:
- âœ… **OpenAI**: Check API key and quota
- âœ… **Hugging Face**: Ensure model can be downloaded
- âœ… Switch providers: `export UML_EMBEDDING_PROVIDER=huggingface`
</details>

<details>
<summary><b>âš¡ Performance Issues</b></summary>

**Problem**: Slow responses or high memory usage

**Solutions**:
- âœ… Use `context_strategy="recent"` for speed
- âœ… Reduce `context_count` parameter
- âœ… Set `UML_MAX_CONTEXT_LENGTH=2000`
</details>

## ğŸ“š Documentation

| Document | Description |
|----------|-------------|
| [ğŸ“– API Reference](docs/API.md) | Complete API documentation |
| [ğŸ’¡ Examples](docs/EXAMPLES.md) | Usage examples and patterns |
| [âš™ï¸ Configuration](docs/CONFIGURATION.md) | Configuration options |
| [ğŸ”§ Troubleshooting](docs/TROUBLESHOOTING.md) | Detailed troubleshooting |

## ğŸŒŸ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=CodeGovindz/MindLayer&type=Date)](https://star-history.com/#CodeGovindz/MindLayer&Date)

## ğŸ™ Acknowledgments

- [OpenAI](https://openai.com) for GPT models and embeddings
- [Google](https://ai.google.dev) for Gemini models  
- [Anthropic](https://anthropic.com) for Claude models
- [Groq](https://groq.com) for fast inference
- [Hugging Face](https://huggingface.co) for open-source embeddings
- [FAISS](https://github.com/facebookresearch/faiss) for vector search

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“ Support

- ğŸ“§ **Email**: support@universalmemorylayer.com
- ğŸ’¬ **Discord**: [Join our community](https://discord.gg/universal-memory-layer)
- ğŸ› **Issues**: [GitHub Issues](https://github.com/CodeGovindz/MindLayer/issues)
- ğŸ“– **Docs**: [Documentation](https://github.com/CodeGovindz/MindLayer/tree/main/docs)

---

<div align="center">

**Made with â¤ï¸ by CodeGovindz**

[![GitHub stars](https://img.shields.io/github/stars/CodeGovindz/MindLayer?style=social)](https://github.com/CodeGovindz/MindLayer/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/CodeGovindz/MindLayer?style=social)](https://github.com/CodeGovindz/MindLayer/network/members)
[![GitHub watchers](https://img.shields.io/github/watchers/CodeGovindz/MindLayer?style=social)](https://github.com/CodeGovindz/MindLayer/watchers)

</div>